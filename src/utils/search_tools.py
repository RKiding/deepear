import os
import hashlib
import json
import re
import requests
import time
import threading
from typing import List, Dict, Optional, Any
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.baidusearch import BaiduSearchTools
from agno.agent import Agent
from loguru import logger
from datetime import datetime
from utils.database_manager import DatabaseManager
from utils.content_extractor import ContentExtractor
from utils.llm.factory import get_model
from utils.hybrid_search import LocalNewsSearch

# é»˜è®¤æœç´¢ç¼“å­˜ TTLï¼ˆç§’ï¼‰ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
DEFAULT_SEARCH_TTL = int(os.getenv("SEARCH_CACHE_TTL", "3600"))  # é»˜è®¤ 1 å°æ—¶


class JinaSearchEngine:
    """Jina Search API å°è£… - ä½¿ç”¨ s.jina.ai è¿›è¡Œç½‘ç»œæœç´¢"""
    
    JINA_SEARCH_URL = "https://s.jina.ai/"
    
    # é€Ÿç‡é™åˆ¶é…ç½®
    _rate_limit_no_key = 10  # æ—  key æ—¶æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
    _rate_window = 60.0
    _min_interval = 2.0
    _request_times = []
    _last_request_time = 0.0
    _lock = threading.Lock()
    
    def __init__(self):
        self.api_key = os.getenv("JINA_API_KEY", "").strip()
        self.has_api_key = bool(self.api_key)
        if self.has_api_key:
            logger.info("âœ… Jina Search API key configured")
    
    @classmethod
    def _wait_for_rate_limit(cls, has_api_key: bool) -> None:
        """ç­‰å¾…ä»¥æ»¡è¶³é€Ÿç‡é™åˆ¶"""
        if has_api_key:
            time.sleep(0.3)
            return
        
        with cls._lock:
            current_time = time.time()
            cls._request_times = [t for t in cls._request_times if current_time - t < cls._rate_window]
            
            if len(cls._request_times) >= cls._rate_limit_no_key:
                oldest = cls._request_times[0]
                wait_time = cls._rate_window - (current_time - oldest) + 1.0
                if wait_time > 0:
                    logger.warning(f"â³ Jina Search rate limit, waiting {wait_time:.1f}s...")
                    time.sleep(wait_time)
                    current_time = time.time()
                    cls._request_times = [t for t in cls._request_times if current_time - t < cls._rate_window]
            
            time_since_last = current_time - cls._last_request_time
            if time_since_last < cls._min_interval:
                time.sleep(cls._min_interval - time_since_last)
            
            cls._request_times.append(time.time())
            cls._last_request_time = time.time()
    
    def search(self, query: str, max_results: int = 5) -> List[Dict]:
        """
        ä½¿ç”¨ Jina Search API æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« title, url, content
        """
        if not query:
            return []
        
        logger.info(f"ğŸ” Jina Search: {query}")
        
        # ç­‰å¾…é€Ÿç‡é™åˆ¶
        self._wait_for_rate_limit(self.has_api_key)
        
        headers = {
            "Accept": "application/json",
            "X-Retain-Images": "none",
        }
        
        if self.has_api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        try:
            # Jina Search API: https://s.jina.ai/{query}
            import urllib.parse
            encoded_query = urllib.parse.quote(query)
            url = f"{self.JINA_SEARCH_URL}{encoded_query}"
            
            response = requests.get(url, headers=headers, timeout=30)
            
            if response.status_code == 429:
                logger.warning("âš ï¸ Jina Search rate limited (429), waiting 30s...")
                time.sleep(30)
                return self.search(query, max_results)
            
            if response.status_code != 200:
                logger.warning(f"Jina Search failed (Status {response.status_code})")
                return []
            
            # è§£æå“åº”
            try:
                data = response.json()
            except json.JSONDecodeError:
                # å¦‚æœè¿”å›çº¯æ–‡æœ¬ï¼Œå°è¯•è§£æ
                data = {"data": [{"title": "Search Result", "url": "", "content": response.text}]}
            
            results = []
            
            # Jina è¿”å›æ ¼å¼å¯èƒ½æ˜¯ {"data": [...]} æˆ–ç›´æ¥æ˜¯åˆ—è¡¨
            items = data.get("data", []) if isinstance(data, dict) else data
            if not isinstance(items, list):
                items = [items] if items else []
            
            for i, item in enumerate(items[:max_results]):
                if isinstance(item, dict):
                    results.append({
                        "title": item.get("title", f"Result {i+1}"),
                        "url": item.get("url", ""),
                        "href": item.get("url", ""),  # å…¼å®¹æ€§
                        "content": item.get("content", item.get("description", "")),
                        "body": item.get("content", item.get("description", "")),  # å…¼å®¹æ€§
                    })
                elif isinstance(item, str):
                    results.append({
                        "title": f"Result {i+1}",
                        "url": "",
                        "content": item
                    })
            
            logger.info(f"âœ… Jina Search returned {len(results)} results")
            return results
            
        except requests.exceptions.Timeout:
            logger.error("Jina Search timeout")
            return []
        except requests.exceptions.RequestException as e:
            logger.error(f"Jina Search request error: {e}")
            return []
        except Exception as e:
            logger.error(f"Jina Search unexpected error: {e}")
            return []

class SearchTools:
    """æ‰©å±•æ€§æœç´¢å·¥å…·åº“ - æ”¯æŒå¤šå¼•æ“èšåˆä¸å†…å®¹ç¼“å­˜"""
    
    def __init__(self, db: DatabaseManager):
        self.db = db
        
        # æ£€æŸ¥ Jina API Key æ˜¯å¦é…ç½®
        jina_api_key = os.getenv("JINA_API_KEY", "").strip()
        self._jina_enabled = bool(jina_api_key)
        
        self._engines = {
            "ddg": DuckDuckGoTools(),
            "baidu": BaiduSearchTools(),
            "local": LocalNewsSearch(db)
        }
        
        # å¦‚æœé…ç½®äº† Jina API Keyï¼Œæ·»åŠ  Jina å¼•æ“
        if self._jina_enabled:
            self._engines["jina"] = JinaSearchEngine()
            logger.info("ğŸš€ Jina Search engine enabled (JINA_API_KEY configured)")
        
        # ç¡®å®šé»˜è®¤æœç´¢å¼•æ“
        self._default_engine = "jina" if self._jina_enabled else "ddg"

    def _generate_hash(self, query: str, engine: str, max_results: int) -> str:
        return hashlib.md5(f"{engine}:{query}:{max_results}".encode()).hexdigest()

    def search(self, query: str, engine: str = None, max_results: int = 5, ttl: Optional[int] = None) -> str:
        """
        ä½¿ç”¨æŒ‡å®šæœç´¢å¼•æ“æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œç»“æœä¼šè¢«ç¼“å­˜ä»¥æé«˜æ•ˆç‡ã€‚
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼Œå¦‚ "è‹±ä¼Ÿè¾¾è´¢æŠ¥" æˆ– "å…‰ä¼è¡Œä¸šæ”¿ç­–"ã€‚
            engine: æœç´¢å¼•æ“é€‰æ‹©ã€‚å¯é€‰å€¼: 
                    "jina" (Jina Searchï¼Œéœ€é…ç½® JINA_API_KEYï¼ŒLLMå‹å¥½è¾“å‡º),
                    "ddg" (DuckDuckGoï¼Œæ¨èè‹±æ–‡/å›½é™…æœç´¢), 
                    "baidu" (ç™¾åº¦ï¼Œæ¨èä¸­æ–‡/å›½å†…æœç´¢),
                    "local" (æœ¬åœ°å†å²æ–°é—»æœç´¢ï¼ŒåŸºäºå‘é‡+BM25)ã€‚
                    é»˜è®¤: è‹¥é…ç½®äº† JINA_API_KEY åˆ™ä½¿ç”¨ "jina"ï¼Œå¦åˆ™ "ddg"ã€‚
            max_results: æœŸæœ›è¿”å›çš„ç»“æœæ•°é‡ï¼Œé»˜è®¤ 5 æ¡ã€‚
            ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰ã€‚å¦‚æœç¼“å­˜è¶…è¿‡æ­¤æ—¶é—´ä¼šé‡æ–°æœç´¢ã€‚
                 é»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡ SEARCH_CACHE_TTL æˆ– 3600 ç§’ã€‚
                 è®¾ä¸º 0 å¯å¼ºåˆ¶åˆ·æ–°ã€‚
        
        Returns:
            æœç´¢ç»“æœçš„æ–‡æœ¬æè¿°ï¼ŒåŒ…å«æ ‡é¢˜ã€æ‘˜è¦å’Œé“¾æ¥ã€‚
        """
        # ä½¿ç”¨é»˜è®¤å¼•æ“ï¼ˆå¦‚æœé…ç½®äº† Jina åˆ™ä¼˜å…ˆä½¿ç”¨ Jinaï¼‰
        if engine is None:
            engine = self._default_engine
        
        if engine not in self._engines:
            return f"Error: Unsupported engine '{engine}'. Available: {list(self._engines.keys())}"

        query_hash = self._generate_hash(query, engine, max_results)
        effective_ttl = ttl if ttl is not None else DEFAULT_SEARCH_TTL
        
        # 1. å°è¯•ä»ç¼“å­˜è¯»å– (local å¼•æ“ä¸ç¼“å­˜ï¼Œå› ä¸ºå®ƒæœ¬èº«å°±æ˜¯æŸ¥åº“)
        if engine != "local":
            cache = self.db.get_search_cache(query_hash, ttl_seconds=effective_ttl if effective_ttl > 0 else None)
            if cache and effective_ttl != 0:
                logger.info(f"â„¹ï¸ Found search results in cache for: {query} ({engine})")
                return cache['results']

        # 2. æ‰§è¡ŒçœŸå®æœç´¢
        logger.info(f"ğŸ“¡ Searching {engine} for: {query}")
        try:
            tool = self._engines[engine]
            if engine == "jina":
                # Jina Search è¿”å› List[Dict]
                jina_results = tool.search(query, max_results=max_results)
                results = []
                for r in jina_results:
                    results.append({
                        "title": r.get("title", ""),
                        "href": r.get("url", ""),
                        "body": r.get("content", "")
                    })
            elif engine == "ddg":
                results = tool.duckduckgo_search(query, max_results=max_results)
            elif engine == "baidu":
                results = tool.baidu_search(query, max_results=max_results)
            elif engine == "local":
                # LocalNewsSearch è¿”å›çš„æ˜¯ List[Dict]
                local_results = tool.search(query, top_n=max_results)
                results = []
                for r in local_results:
                    results.append({
                        "title": r.get("title"),
                        "href": r.get("url", "local"),
                        "body": r.get("content", "")
                    })
            else:
                results = "Search not implemented for this engine."
            
            results_str = str(results)
            if engine != "local":
                self.db.save_search_cache(query_hash, query, engine, results_str)
            return results_str
            
        except Exception as e:
            # æœç´¢å¤±è´¥æ—¶çš„é™çº§ç­–ç•¥
            if engine == "jina":
                logger.warning(f"âš ï¸ Jina search failed, falling back to ddg: {query} ({e})")
                try:
                    return self.search(query, engine="ddg", max_results=max_results, ttl=ttl)
                except Exception as e2:
                    logger.error(f"âŒ DDG fallback also failed for {query}: {e2}")
            elif engine == "ddg":
                logger.warning(f"âš ï¸ DDG search failed, falling back to baidu: {query} ({e})")
                try:
                    return self.search(query, engine="baidu", max_results=max_results, ttl=ttl)
                except Exception as e2:
                    logger.error(f"âŒ Baidu fallback also failed for {query}: {e2}")

            logger.error(f"âŒ Search failed for {query}: {e}")
            return f"Error occurred during search: {str(e)}"

    def search_list(self, query: str, engine: str = None, max_results: int = 5, ttl: Optional[int] = None, enrich: bool = True) -> List[Dict]:
        """
        æ‰§è¡Œæœç´¢å¹¶è¿”å›ç»“æ„åŒ–åˆ—è¡¨ (List[Dict])ã€‚
        Dict åŒ…å«: title, href (or url), body (or snippet)
        
        Args:
            engine: æœç´¢å¼•æ“ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„é»˜è®¤å¼•æ“ï¼ˆJina ä¼˜å…ˆï¼‰
            enrich: æ˜¯å¦æŠ“å–æ­£æ–‡å†…å®¹ (é»˜è®¤ True)
        """
        # ä½¿ç”¨é»˜è®¤å¼•æ“
        if engine is None:
            engine = self._default_engine
            
        if engine not in self._engines:
            logger.error(f"Unsupported engine {engine}")
            return []
            
        # ä¸åŒçš„ hash ä»¥åŒºåˆ†æ˜¯å¦ enrichment
        enrich_suffix = ":enriched" if enrich else ""
        query_hash = self._generate_hash(query, engine + enrich_suffix, max_results)
        effective_ttl = ttl if ttl is not None else DEFAULT_SEARCH_TTL
        
        # 1. å°è¯•ä»ç¼“å­˜è¯»å–
        cache = self.db.get_search_cache(query_hash, ttl_seconds=effective_ttl if effective_ttl > 0 else None)
        if cache and effective_ttl != 0:
            try:
                cached_data = json.loads(cache['results'])
                if isinstance(cached_data, list):
                    logger.info(f"â„¹ï¸ Found structured search cache for: {query}")
                    return cached_data
            except:
                pass
        
        # 1.5 Smart Cache (Fuzzy + LLM)
        if effective_ttl != 0:
            try:
                # 1. Similar cached queries
                similar_queries = self.db.find_similar_queries(query, limit=3)
                # Filter by TTL
                valid_candidates = []
                for q in similar_queries:
                    if q['query'] == query: continue 
                    q_time = datetime.fromisoformat(q['timestamp'])
                    if effective_ttl and (datetime.now() - q_time).total_seconds() > effective_ttl:
                        continue
                    q['type'] = 'cached_search'
                    valid_candidates.append(q)

                # 2. Relevant local news (as search results)
                local_news = self.db.search_local_news(query, limit=3)
                if local_news:
                    # Group local news as a single "candidate" source? Or individual?
                    # Better to treat "Local News Database" as one candidate source that contains X items.
                    # Or just add them to candidates list?
                    # Let's package strictly relevant news as a "local_news_bundle"
                    valid_candidates.append({
                        'type': 'local_news',
                        'query': 'Local Database News',
                        'items': local_news,
                        'timestamp': datetime.now().isoformat()
                    })
                
                if valid_candidates:
                    logger.info(f"ğŸ¤” Found {len(valid_candidates)} smart cache candidates (Queries/News). Asking LLM...")
                    evaluation = self._evaluate_cache_relevance(query, valid_candidates)
                    
                    if evaluation and evaluation.get('reuse', False):
                        idx = evaluation.get('index', -1)
                        if 0 <= idx < len(valid_candidates):
                            chosen = valid_candidates[idx]
                            logger.info(f"ğŸ¤– LLM suggested reusing: '{chosen.get('query')}' ({chosen['type']})")
                            
                            if chosen['type'] == 'cached_search':
                                # Load the chosen cache
                                cache = self.db.get_search_cache(chosen['query_hash']) 
                                if cache:
                                    try:
                                        cached_data = json.loads(cache['results'])
                                        if isinstance(cached_data, list):
                                            return cached_data
                                    except:
                                        pass
                            elif chosen['type'] == 'local_news':
                                # Convert local news items to search result format
                                news_results = []
                                for i, news in enumerate(chosen['items'], 1):
                                    news_results.append({
                                        "id": news.get('id'),
                                        "rank": i,
                                        "title": news.get('title'),
                                        "url": news.get('url'),
                                        "content": news.get('content'),
                                        "original_snippet": news.get('content')[:200] if news.get('content') else '',
                                        "source": f"Local News ({news.get('source')})",
                                        "publish_time": news.get('publish_time'),
                                        "crawl_time": news.get('crawl_time'),
                                        "sentiment_score": news.get('sentiment_score', 0),
                                        "meta_data": {"origin": "local_db"}
                                    })
                                return news_results

            except Exception as e:
                logger.warning(f"Smart cache check failed: {e}")
        
        # 2. æ‰§è¡Œæœç´¢
        logger.info(f"ğŸ“¡ Searching {engine} (structured) for: {query}")
        try:
            tool = self._engines[engine]
            results = []
            if engine == "jina":
                # Jina Search ç›´æ¥è¿”å›ç»“æ„åŒ–æ•°æ®
                jina_results = tool.search(query, max_results=max_results)
                for r in jina_results:
                    results.append({
                        "title": r.get("title", ""),
                        "url": r.get("url", ""),
                        "href": r.get("url", ""),
                        "body": r.get("content", ""),
                        "content": r.get("content", ""),
                        "source": "Jina Search"
                    })
            elif engine == "ddg":
                results = tool.duckduckgo_search(query, max_results=max_results)
            elif engine == "baidu":
                results = tool.baidu_search(query, max_results=max_results)
            elif engine == "local":
                # LocalNewsSearch è¿”å›çš„æ˜¯ List[Dict]
                local_results = tool.search(query, top_n=max_results)
                results = []
                for r in local_results:
                    results.append({
                        "title": r.get("title"),
                        "url": r.get("url", "local"),
                        "body": r.get("content", "")[:500],
                        "source": f"Local ({r.get('source', 'db')})",
                        "publish_time": r.get("publish_time")
                    })
            
            # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ JSON è¿”å› (Baidu å¸¸è¿” JSON å­—ç¬¦ä¸²)
            if isinstance(results, str) and engine not in ["local", "jina"]:
                try:
                    results = json.loads(results)
                except:
                    pass
            
            # è½¬ä¸ºç»Ÿä¸€æ ¼å¼
            normalized_results = []
            if isinstance(results, list):
                
                for i, r in enumerate(results, 1):
                    title = r.get('title', '')
                    url = r.get('href') or r.get('url') or r.get('link', '')
                    content = r.get('body') or r.get('snippet') or r.get('abstract', '')
                    
                    if title and url:
                        normalized_results.append({
                            "id": self._generate_hash(url + query, "search_item", i),
                            "rank": i,
                            "title": title,
                            "url": url,
                            "content": content,
                            "original_snippet": content, # ä¿ç•™æ‘˜è¦
                            "source": f"Search ({engine})",
                            "publish_time": datetime.now().isoformat(), # æš‚ç”¨å½“å‰æ—¶é—´
                            "crawl_time": datetime.now().isoformat(),
                            "meta_data": {"query": query, "engine": engine}
                        })
            
            # Fallback if still string and failed to parse
            elif isinstance(results, str) and results:
                 normalized_results.append({"title": query, "url": "", "content": results, "source": engine})

            # 3. æŠ“å–æ­£æ–‡ & è®¡ç®—æƒ…ç»ª (Enrichment)
            # æ³¨æ„ï¼šå¦‚æœä½¿ç”¨ Jina Searchï¼Œå†…å®¹å·²ç»æ˜¯ LLM å‹å¥½æ ¼å¼ï¼Œå¯é€‰æ‹©è·³è¿‡ enrichment
            skip_content_enrichment = (engine == "jina")
            
            if enrich and normalized_results:
                logger.info(f"ğŸ•¸ï¸ Enriching {len(normalized_results)} search results with Jina & Sentiment...")
                extractor = ContentExtractor()
                
                # Lazy load sentiment tool
                if not hasattr(self, 'sentiment_tool') or self.sentiment_tool is None:
                    from utils.sentiment_tools import SentimentTools
                    self.sentiment_tool = SentimentTools(self.db)
                
                for item in normalized_results:
                    if item.get("url"):
                        try:
                            # å¦‚æœæ˜¯ Jina Searchï¼Œå†…å®¹å·²ç»è¶³å¤Ÿå¥½ï¼Œè·³è¿‡é¢å¤–æŠ“å–
                            if skip_content_enrichment and item.get("content") and len(item.get("content", "")) > 100:
                                full_content = item["content"]
                            else:
                                # Use Jina Reader to get full content
                                full_content = extractor.extract_with_jina(item["url"], timeout=60)
                            
                            if full_content and len(full_content) > 100:
                                item["content"] = full_content
                                
                                # Calculate sentiment
                                # Use title + snippet of content for efficiency
                                text_to_analyze = f"{item['title']} {full_content[:500]}"
                                sent_result = self.sentiment_tool.analyze_sentiment(text_to_analyze)  # Using self.sentiment_tool
                                score = sent_result.get('score', 0.0)
                                item["sentiment_score"] = float(score)
                                
                                logger.info(f"  âœ… Enriched: {item['title'][:20]}... (Sentiment: {score:.2f})")
                            else:
                                # Fallback: Use snippet for sentiment
                                logger.info(f"  âš ï¸ Content short/failed for {item['url']}, using snippet for sentiment.")
                                text_to_analyze = f"{item['title']} {item['content']}" # content is snippet here
                                sent_result = self.sentiment_tool.analyze_sentiment(text_to_analyze)
                                score = sent_result.get('score', 0.0)
                                item["sentiment_score"] = float(score)

                        except Exception as e:
                             # Fallback: Use snippet for sentiment on error
                            logger.warning(f"Failed to enrich {item['url']}: {e}. Using snippet.")
                            text_to_analyze = f"{item['title']} {item['content']}"
                            sent_result = self.sentiment_tool.analyze_sentiment(text_to_analyze)
                            score = sent_result.get('score', 0.0)
                            item["sentiment_score"] = float(score)
            
            # ç¼“å­˜ç»“æœ list
            if normalized_results:
                # Pass list directly, DB manager will handle JSON dump for main cache and populate search_details
                # Only cache if NOT from local news reuse (though this logic path is for fresh search)
                self.db.save_search_cache(query_hash, query, engine, normalized_results)
            
            return normalized_results
            
        except Exception as e:
            # æœç´¢å¤±è´¥æ—¶çš„é™çº§ç­–ç•¥
            if engine == "jina":
                logger.warning(f"âš ï¸ Jina search_list failed, falling back to ddg: {query} ({e})")
                try:
                    return self.search_list(query, engine="ddg", max_results=max_results, ttl=ttl, enrich=enrich)
                except Exception as e2:
                    logger.error(f"âŒ DDG fallback (search_list) also failed for {query}: {e2}")
            elif engine == "ddg":
                logger.warning(f"âš ï¸ DDG search_list failed, falling back to baidu: {query} ({e})")
                try:
                    return self.search_list(query, engine="baidu", max_results=max_results, ttl=ttl, enrich=enrich)
                except Exception as e2:
                    logger.error(f"âŒ Baidu fallback (search_list) also failed for {query}: {e2}")

            logger.error(f"âŒ Structured search failed for {query}: {e}")
            return []

    def _evaluate_cache_relevance(self, current_query: str, candidates: List[Dict]) -> Dict:
        """
        ä½¿ç”¨ LLM è¯„ä¼°ç¼“å­˜å€™é€‰æ˜¯å¦è¶³ä»¥å›ç­”å½“å‰é—®é¢˜ã€‚
        """
        try:
            # Prepare candidates text
            candidates_desc = []
            for i, c in enumerate(candidates):
                if c['type'] == 'cached_search':
                    # Preview cached results if available? 
                    # Maybe just use the query string as a proxy for what's in there.
                    # Or peek at 'results' snippet.
                    preview = ""
                    try:
                         # Attempt to peek first result title from JSON string
                         # Note: c.get('results') might be a stringified JSON list
                         res_list = json.loads(c.get('results', '[]'))
                         if res_list and isinstance(res_list, list) and len(res_list) > 0:
                             first_item = res_list[0]
                             if isinstance(first_item, dict) and 'title' in first_item:
                                 preview = f" (Contains: {first_item.get('title', '')[:50]}...)"
                    except:
                        pass
                    candidates_desc.append(f"[{i}] Old Search Query: '{c['query']}' {preview} (Time: {c['timestamp']})")
                elif c['type'] == 'local_news':
                     # List titles of local news
                     titles = [item['title'] for item in c['items'][:3]]
                     candidates_desc.append(f"[{i}] Local Database News: {', '.join(titles)}... (Time: {c['timestamp']})")

            prompt = f"""
            Task: Decide if existing information is sufficient for the new search query.
            
            New Query: "{current_query}"
            
            Available Information Candidates:
            {chr(10).join(candidates_desc)}
            
            Instructions:
            1. Analyze if any candidate provides ENOUGH up-to-date info for the "New Query".
            2. If yes, choose the best one.
            3. If the query implies needing LATEST real-time info and candidates are old, choose none.
            4. Return strictly JSON: {{"reuse": true/false, "index": <candidate_index_int>, "reason": "short explanation"}}
            """
            # åˆå§‹åŒ–æ¨¡å‹
            provider = os.getenv("LLM_PROVIDER", "ust")
            model_id = os.getenv("LLM_MODEL", "Qwen")
            host = os.getenv("LLM_HOST")
            if host:
                model = get_model(provider, model_id, host=host)
            else:
                model = get_model(provider, model_id)
                
            agent = Agent(model=model, markdown=False)
            
            response = agent.run(prompt)
            content = response.content
            
            # Parse JSON
            json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            elif '{' in content:
                 # Fallback for cases where LLM doesn't wrap in ```json
                 return json.loads(content[content.find('{'):content.rfind('}')+1])
            return {"reuse": False}
            
        except Exception as e:
            logger.warning(f"LLM evaluation failed: {e}")
            return {"reuse": False}

    def aggregate_search(self, query: str, engines: Optional[List[str]] = None, max_results: int = 5) -> str:
        """
        ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“åŒæ—¶æœç´¢å¹¶èšåˆç»“æœï¼Œè·å¾—æ›´å…¨é¢çš„ä¿¡æ¯è¦†ç›–ã€‚
        
        Args:
            query: æœç´¢å…³é”®è¯ã€‚
            engines: è¦ä½¿ç”¨çš„æœç´¢å¼•æ“åˆ—è¡¨ã€‚å¯é€‰å€¼: ["ddg", "baidu"]ã€‚
                     é»˜è®¤åŒæ—¶ä½¿ç”¨ ddg å’Œ baiduã€‚
            max_results: æ¯ä¸ªå¼•æ“æœŸæœ›è¿”å›çš„ç»“æœæ•°é‡ã€‚
        
        Returns:
            èšåˆåçš„æœç´¢ç»“æœï¼ŒæŒ‰å¼•æ“åˆ†ç»„æ˜¾ç¤ºã€‚
        """
        engines = engines or ["ddg", "baidu"]
        aggregated_results = []
        for engine in engines:
            res = self.search(query, engine=engine, max_results=max_results)
            aggregated_results.append(f"--- Results from {engine.upper()} ---\n{res}")
        
        return "\n\n".join(aggregated_results)
