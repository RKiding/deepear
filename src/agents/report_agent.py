import hashlib
import json
import textwrap
import time
from datetime import datetime, timedelta
import pandas as pd
from typing import List, Dict, Any, Optional
from agno.agent import Agent
from agno.models.base import Model
from loguru import logger
from types import SimpleNamespace

from utils.database_manager import DatabaseManager
from utils.hybrid_search import InMemoryRAG
from utils.json_utils import extract_json
from utils.stock_tools import StockTools
import re
from schema.models import InvestmentSignal, InvestmentReport, TransmissionNode, ClusterContext, ForecastResult
from agents.forecast_agent import ForecastAgent
from prompts.report_agent import (
    get_report_planner_base_instructions,
    get_report_writer_base_instructions,
    get_report_editor_base_instructions,
    format_signal_for_report,
    get_cluster_planner_instructions,
    get_report_planner_instructions,
    get_report_writer_instructions,
    get_report_editor_instructions,
    get_section_editor_instructions,
    get_summary_generator_instructions,
    get_final_assembly_instructions,
    get_cluster_task,
    get_writer_task,
    get_planner_task,
    get_editor_task
)


class ReportAgent:
    """
    ç ”æŠ¥ç”Ÿæˆå™¨ (ReportAgent) - Map-Reduce æ¶æ„
    æ”¯æŒå¢é‡ç¼–è¾‘æ¨¡å¼ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ç« èŠ‚
    """
    
    def __init__(self, db: DatabaseManager, model: Model, incremental_edit: bool = True, tool_model: Optional[Model] = None):
        self.db = db
        self.model = model
        self.tool_model = tool_model or model
        self.incremental_edit = incremental_edit
        
        # 0. InMemory RAG for cross-chapter context
        self.rag = InMemoryRAG(data=[], text_fields=["title", "content", "summary"])
        
        # 1. Planner Agent
        self.planner = Agent(
            model=self.tool_model,
            tools=[self.rag.search],
            instructions=[get_report_planner_base_instructions()],
            markdown=False,
            debug_mode=True,
            output_schema=ClusterContext if hasattr(self.tool_model, 'response_format') else None
        )
        
        # 2. Writer Agent
        self.writer = Agent(
            model=model,
            instructions=[get_report_writer_base_instructions()],
            markdown=False,
            debug_mode=True
        )
        
        # 3. Editor Agent
        self.editor = Agent(
            model=self.tool_model,
            tools=[self.rag.search],
            instructions=[get_report_editor_base_instructions()],
            markdown=False,
            debug_mode=True
        )
        
        # 5. Section Editor Agent (ç”¨äºå¢é‡ç¼–è¾‘)
        self.section_editor = Agent(
            model=self.tool_model,
            tools=[self.rag.search],
            instructions=[get_report_editor_base_instructions()],
            markdown=False,
            debug_mode=True
        )
        
        # 6. Forecast Agent (lazy init: avoid heavy Kronos load unless actually requested)
        self._forecast_agent: Optional[ForecastAgent] = None
        
        logger.info(f"ğŸ“ ReportAgent initialized (incremental_edit={incremental_edit})")

    def _get_forecast_agent(self) -> ForecastAgent:
        if self._forecast_agent is None:
            self._forecast_agent = ForecastAgent(self.db, self.model)
        return self._forecast_agent

    @staticmethod
    def _make_cite_key(url: str, title: str = "", source_name: str = "") -> str:
        basis = (url or "").strip() or f"{(title or '').strip()}|{(source_name or '').strip()}"
        digest = hashlib.sha1(basis.encode("utf-8")).hexdigest()[:8]
        return f"SF-{digest}"

    def _build_bibliography(self, signals: List[Any]) -> tuple[list[Dict[str, Any]], Dict[int, list[str]]]:
        """Build stable bibliography entries and per-signal cite key mapping.

        Returns:
            bib_entries: ordered unique entries: [{key,url,title,source,publish_time}]
            signal_to_keys: {signal_index(1-based): [key1,key2,...]}
        """
        bib_by_key: Dict[str, Dict[str, Any]] = {}
        signal_to_keys: Dict[int, list[str]] = {}

        for sig_idx, signal in enumerate(signals, 1):
            source_items: list[Dict[str, Any]] = []

            if hasattr(signal, "sources") and getattr(signal, "sources"):
                source_items = list(getattr(signal, "sources") or [])
            elif isinstance(signal, dict) and signal.get("sources"):
                # analyzed_signals are dicts; their sources are nested under the `sources` key
                src_list = signal.get("sources")
                if isinstance(src_list, list) and src_list:
                    source_items = list(src_list)
            elif isinstance(signal, dict):
                # Treat raw signals as single-source entries
                if signal.get("url") or signal.get("title"):
                    source_items = [
                        {
                            "title": signal.get("title"),
                            "url": signal.get("url"),
                            "source_name": signal.get("source") or signal.get("source_name"),
                            "publish_time": signal.get("publish_time"),
                        }
                    ]

            if not source_items:
                continue

            for src in source_items:
                url = (src.get("url") or "").strip()
                title = (src.get("title") or "").strip()
                source_name = (src.get("source_name") or src.get("source") or "").strip()
                publish_time = (src.get("publish_time") or "").strip() if isinstance(src.get("publish_time"), str) else src.get("publish_time")

                key = self._make_cite_key(url=url, title=title, source_name=source_name)
                signal_to_keys.setdefault(sig_idx, [])
                if key not in signal_to_keys[sig_idx]:
                    signal_to_keys[sig_idx].append(key)

                if key in bib_by_key:
                    continue

                # Prefer canonical metadata from DB when possible
                enriched = self.db.lookup_reference_by_url(url) if url else None
                bib_by_key[key] = {
                    "key": key,
                    "url": url or (enriched.get("url") if enriched else ""),
                    "title": (enriched.get("title") if enriched else None) or title or "ï¼ˆæ— æ ‡é¢˜ï¼‰",
                    "source": (enriched.get("source") if enriched else None) or source_name or "ï¼ˆæœªçŸ¥æ¥æºï¼‰",
                    "publish_time": (enriched.get("publish_time") if enriched else None) or publish_time or "",
                }

        return list(bib_by_key.values()), signal_to_keys

    @staticmethod
    def _render_references_section(bib_entries: list[Dict[str, Any]], key_to_num: Dict[str, int]) -> str:
        lines = ["## å‚è€ƒæ–‡çŒ®", ""]
        if not bib_entries:
            lines.append("ï¼ˆæ— ï¼‰")
            return "\n".join(lines).strip() + "\n"

        for entry in bib_entries:
            key = entry.get("key")
            num = key_to_num.get(key) if key else None
            title = entry.get("title") or "ï¼ˆæ— æ ‡é¢˜ï¼‰"
            source = entry.get("source") or "ï¼ˆæœªçŸ¥æ¥æºï¼‰"
            url = entry.get("url") or ""
            publish_time = entry.get("publish_time") or ""
            suffix = ""
            if publish_time:
                suffix = f"ï¼Œ{publish_time}"
            label = f"[{num}]" if isinstance(num, int) else "[?]"
            if url:
                lines.append(f"<a id=\"ref-{key}\"></a>{label} {title} ({source}{suffix}), {url}")
            else:
                lines.append(f"<a id=\"ref-{key}\"></a>{label} {title} ({source}{suffix})")

        return "\n".join(lines).strip() + "\n"

    @staticmethod
    def _inject_references(report_md: str, references_md: str) -> str:
        # Replace existing references section, if any
        pattern = re.compile(r"(?ms)^##\s*å‚è€ƒæ–‡çŒ®\s*$.*?(?=^##\s|\Z)")
        if pattern.search(report_md or ""):
            return pattern.sub(references_md.strip() + "\n\n", report_md).strip()

        # Otherwise append at end
        return (report_md or "").rstrip() + "\n\n" + references_md.strip() + "\n"

    @staticmethod
    def _normalize_citations(report_md: str, signal_to_keys: Dict[int, list[str]], key_to_num: Dict[str, int]) -> str:
        text = report_md or ""

        # Convert legacy [[n]] to the first available cite key for that signal.
        def repl_legacy(match: re.Match) -> str:
            idx = int(match.group(1))
            keys = signal_to_keys.get(idx) or []
            if not keys:
                return match.group(0)
            key = keys[0]
            num = key_to_num.get(key)
            label = f"[{num}]" if isinstance(num, int) else "[?]"
            return f"{label}(#ref-{key})"

        text = re.sub(r"\[\[(\d+)\]\]", repl_legacy, text)

        # Convert cite keys to numbered display while keeping stable anchor: [@KEY] -> [N](#ref-KEY)
        def repl_key(match: re.Match) -> str:
            key = match.group("key")
            num = key_to_num.get(key)
            label = f"[{num}]" if isinstance(num, int) else "[?]"
            return f"{label}(#ref-{key})"

        text = re.sub(r"\[@(?P<key>[A-Za-z0-9][A-Za-z0-9:_\-]{0,64})\](?!\()", repl_key, text)

        # Convert loose cite markers like: ï¼ˆ@SF-xxxxxxxxï¼‰ / (@SF-xxxxxxxx)
        # These sometimes appear when the model forgets the bracket form.
        def repl_loose_key(match: re.Match) -> str:
            lparen = match.group("lparen")
            rparen = match.group("rparen")
            key = match.group("key")
            num = key_to_num.get(key)
            label = f"[{num}]" if isinstance(num, int) else "[?]"
            return f"{lparen}{label}(#ref-{key}){rparen}"

        text = re.sub(
            r"(?P<lparen>[\(\ï¼ˆ])\s*@(?P<key>SF-[0-9a-fA-F]{8})\s*(?P<rparen>[\)\ï¼‰])",
            repl_loose_key,
            text,
        )

        return text

    @staticmethod
    def _clean_ticker(ticker_raw: str) -> str:
        t = (ticker_raw or "").strip()
        if not t:
            return ""
        if "," in t:
            t = t.split(",")[0].strip()
        if "." in t:
            t = t.split(".")[0].strip()
        digits = "".join([c for c in t if c.isdigit()])
        return digits or t

    @classmethod
    def _signal_mentions_ticker(cls, signal: Any, ticker_digits: str) -> bool:
        if not ticker_digits:
            return False

        def norm(s: str) -> str:
            return cls._clean_ticker(s)

        try:
            # Prefer structured impact_tickers if present
            impact = getattr(signal, 'impact_tickers', None) if not isinstance(signal, dict) else signal.get('impact_tickers')
            if isinstance(impact, list):
                for item in impact:
                    if not isinstance(item, dict):
                        continue
                    t = item.get('ticker') or item.get('code') or item.get('symbol')
                    if t and norm(str(t)) == ticker_digits:
                        return True

            # Fallback to text search
            title_text = getattr(signal, 'title', '') if not isinstance(signal, dict) else signal.get('title', '')
            summary_text = getattr(signal, 'summary', '') if not isinstance(signal, dict) else signal.get('summary', '')
            analysis_text = getattr(signal, 'analysis', '') if not isinstance(signal, dict) else signal.get('analysis', '')
            combined = f"{title_text} {summary_text} {analysis_text}"
            return ticker_digits in combined
        except Exception:
            return False

    def _extract_forecast_requests(self, text: str, context_window_chars: int = 1200) -> List[Dict[str, Any]]:
        """Extract forecast requests from markdown content.

        Returns list of dicts: {ticker, pred_len, title, context_snippet}
        """
        if not text:
            return []

        pattern = re.compile(r'```json-chart\s*(\{.*?\})\s*```', re.DOTALL)
        requests: List[Dict[str, Any]] = []

        for match in pattern.finditer(text):
            json_str = match.group(1).strip()
            json_str = (
                json_str.replace("\u201c", '"')
                .replace("\u201d", '"')
                .replace("\u2018", "'")
                .replace("\u2019", "'")
                .replace("â€œ", '"')
                .replace("â€", '"')
                .replace("â€˜", "'")
                .replace("â€™", "'")
            )
            cfg = extract_json(json_str)
            if not cfg:
                continue
            if cfg.get('type') != 'forecast':
                continue

            ticker_raw = str(cfg.get('ticker', '')).strip()
            ticker = self._clean_ticker(ticker_raw)
            if not (ticker.isdigit() and len(ticker) in (5, 6)):
                continue

            try:
                pred_len = int(cfg.get('pred_len', 5))
            except Exception:
                pred_len = 5
            pred_len = max(1, min(pred_len, 20))

            title = str(cfg.get('title') or f"{ticker_raw} é¢„æµ‹").strip()

            # Prefer writer-provided final attribution over raw surrounding snippet.
            # This supports the workflow: multi-scenario discussion inæ­£æ–‡ -> final chosen scenario -> render ONE forecast chart.
            structured_lines: List[str] = []
            selected_scenario = cfg.get('selected_scenario') or cfg.get('scenario') or cfg.get('case')
            selection_reason = cfg.get('selection_reason') or cfg.get('case_reason') or cfg.get('reason')
            scenarios = cfg.get('scenarios')

            if selected_scenario:
                structured_lines.append(f"- æœ€å¯èƒ½æƒ…æ™¯: {str(selected_scenario).strip()}")
            if selection_reason:
                structured_lines.append(f"- å½’å› : {str(selection_reason).strip()}")
            if isinstance(scenarios, list) and scenarios:
                structured_lines.append("- å¤‡é€‰æƒ…æ™¯:")
                for item in scenarios[:6]:
                    if not isinstance(item, dict):
                        continue
                    name = str(item.get('name', '')).strip()
                    desc = str(item.get('description', '')).strip()
                    prob = item.get('probability', None)
                    prob_str = ""
                    try:
                        if prob is not None:
                            prob_str = f" (p={float(prob):.2f})"
                    except Exception:
                        prob_str = ""
                    line = "  - " + (name or "ï¼ˆæœªå‘½åï¼‰")
                    if desc:
                        line += f": {desc}"
                    line += prob_str
                    structured_lines.append(line)

            structured_context = ""
            if structured_lines:
                structured_context = "ã€æœ€ç»ˆå½’å› /æƒ…æ™¯é€‰æ‹©ï¼ˆä½œè€…åœ¨ forecast å—ä¸­ç»™å®šï¼‰ã€‘\n" + "\n".join(structured_lines)

            start = max(0, match.start() - context_window_chars)
            end = min(len(text), match.end() + context_window_chars)
            snippet = text[start:end]
            # remove the code block itself from the snippet to reduce noise
            snippet = snippet.replace(match.group(0), "").strip()
            # remove any other json-chart blocks to avoid polluting forecast context
            snippet = re.sub(r'```json-chart[\s\S]*?```', '', snippet).strip()

            # If structured attribution exists, use it as the primary snippet; keep raw snippet as fallback.
            context_snippet = structured_context or snippet
            if len(context_snippet) > 3500:
                context_snippet = context_snippet[:3500] + "\n\nï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿å·²æˆªæ–­ï¼‰"

            requests.append({
                'ticker': ticker,
                'ticker_raw': ticker_raw,
                'pred_len': pred_len,
                'title': title,
                'context_snippet': context_snippet,
            })

        return requests

    def _build_forecast_map(self, report_text: str, signals: Optional[List[Any]] = None) -> Dict[tuple, ForecastResult]:
        """Generate forecasts once per unique (ticker, pred_len) to ensure consistency across the report."""
        reqs = self._extract_forecast_requests(report_text)
        if not reqs:
            return {}

        # Allowlist: only generate forecasts for tickers that are backed by structured signals.
        allowed_tickers: Optional[set[str]] = None
        if signals:
            allowed_tickers = set()
            for s in signals:
                impact = getattr(s, 'impact_tickers', None) if not isinstance(s, dict) else s.get('impact_tickers')
                if not isinstance(impact, list):
                    continue
                for item in impact:
                    if not isinstance(item, dict):
                        continue
                    t = item.get('ticker') or item.get('code') or item.get('symbol')
                    tt = self._clean_ticker(str(t or ""))
                    if tt and tt.isdigit() and len(tt) in (5, 6):
                        allowed_tickers.add(tt)
            if not allowed_tickers:
                allowed_tickers = None

        # group by key, merge context
        grouped: Dict[tuple, Dict[str, Any]] = {}
        for r in reqs:
            key = (r['ticker'], int(r['pred_len']))
            g = grouped.get(key)
            if not g:
                grouped[key] = {
                    'ticker': r['ticker'],
                    'pred_len': int(r['pred_len']),
                    'titles': {r['title']},
                    'snippets': [r.get('context_snippet', '')],
                }
            else:
                g['titles'].add(r['title'])
                sn = r.get('context_snippet', '')
                if sn and sn not in g['snippets']:
                    g['snippets'].append(sn)

        logger.info(f"ğŸ”® Forecast requests: total={len(reqs)}, unique={len(grouped)}")

        forecasts: Dict[tuple, ForecastResult] = {}
        for key, g in grouped.items():
            ticker, pred_len = key

            if allowed_tickers is not None and str(ticker) not in allowed_tickers:
                logger.info(f"â„¹ï¸ Skip forecast for {ticker}: not in validated impact_tickers")
                continue

            related_signals: List[Any] = []
            if signals:
                for s in signals:
                    if self._signal_mentions_ticker(s, str(ticker)):
                        related_signals.append(s)

            # If we have signals context, require at least one related signal for attribution.
            if signals and not related_signals:
                logger.info(f"â„¹ï¸ Skip forecast for {ticker}: no attributable signals")
                continue

            # merge context snippets (cap size)
            merged_snippet = "\n\n---\n\n".join([s for s in g['snippets'] if s])
            if len(merged_snippet) > 3500:
                merged_snippet = merged_snippet[:3500] + "\n\nï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿å·²æˆªæ–­ï¼‰"

            extra_context = ""
            if merged_snippet:
                extra_context = (
                    "ã€æŠ¥å‘Šå†™ä½œä¸Šä¸‹æ–‡ï¼ˆæ¥è‡ªç« èŠ‚æ­£æ–‡ï¼Œå¯èƒ½åŒ…å«ä¸»è§‚åˆ¤æ–­ï¼‰ã€‘\n"
                    + merged_snippet
                )

            try:
                fc = self._get_forecast_agent().generate_forecast(
                    str(ticker),
                    related_signals,
                    pred_len=int(pred_len),
                    extra_context=extra_context
                )
                if fc:
                    forecasts[key] = fc
            except Exception as e:
                logger.warning(f"âš ï¸ Forecast generation failed for {ticker} pred_len={pred_len}: {e}")

        return forecasts

    @staticmethod
    def _sanitize_json_chart_blocks(text: str) -> str:
        """Best-effort repair for malformed json-chart fenced blocks.

        Common failure mode: model outputs an opening ```json-chart but forgets to close it.
        That causes downstream chart processing to miss it and swallows the rest of the report.

        Strategy:
        - For each opening fence, locate the first JSON object and close the fence right after
          the JSON object (balanced braces, ignoring braces inside strings).
        - If a closing fence already exists after the JSON object, leave as-is.
        """

        if not text:
            return text

        # Phase 0: Normalize malformed json-chart fences.
        # We only touch fences in/around json-chart blocks to avoid modifying other markdown.
        if "json-chart" in text:
            lines = text.splitlines()
            out_lines: list[str] = []
            in_chart = False
            i = 0
            while i < len(lines):
                line = lines[i]
                stripped = line.strip()

                if not in_chart:
                    # Opening fence variants
                    if stripped in ("```json-chart", "``json-chart", "``` json-chart", "`` json-chart"):
                        out_lines.append("```json-chart")
                        in_chart = True
                        i += 1
                        continue

                    # Variant: opening fence on its own line, language on next line.
                    #   ```
                    #   json-chart
                    #   { ... }
                    if stripped in ("```", "``") and i + 1 < len(lines) and lines[i + 1].strip() == "json-chart":
                        out_lines.append("```json-chart")
                        in_chart = True
                        i += 2
                        continue

                    # Variant: opening fence appears at end of a content line.
                    #   ...ï¼š   ```
                    #   json-chart
                    if stripped.endswith("```") and not stripped.startswith("```"):
                        if i + 1 < len(lines) and lines[i + 1].strip() == "json-chart":
                            prefix = line[: line.rfind("```")].rstrip()
                            if prefix:
                                out_lines.append(prefix)
                            out_lines.append("```json-chart")
                            in_chart = True
                            i += 2
                            continue

                else:
                    # Closing fence variants
                    if stripped in ("```", "``"):
                        out_lines.append("```")
                        in_chart = False
                        i += 1
                        continue

                    # Variant: closing fence appears on the same line after JSON.
                    #   { ... } ```
                    if "```" in line:
                        pos = line.find("```")
                        before = line[:pos].rstrip()
                        after = line[pos + 3 :].strip()
                        if before:
                            out_lines.append(before)
                        out_lines.append("```")
                        in_chart = False
                        if after:
                            out_lines.append(after)
                        i += 1
                        continue

                out_lines.append(line)
                i += 1

            text = "\n".join(out_lines)

        def find_json_end(s: str, start_idx: int) -> Optional[int]:
            # find first '{'
            i = s.find('{', start_idx)
            if i == -1:
                return None
            depth = 0
            in_str = False
            escape = False
            quote = '"'
            for j in range(i, len(s)):
                ch = s[j]
                if in_str:
                    if escape:
                        escape = False
                        continue
                    if ch == '\\':
                        escape = True
                        continue
                    if ch == quote:
                        in_str = False
                    continue

                if ch == '"' or ch == "'":
                    in_str = True
                    quote = ch
                    continue
                if ch == '{':
                    depth += 1
                elif ch == '}':
                    depth -= 1
                    if depth == 0:
                        return j
            return None

        # Phase 1: Repair missing closing fences for properly-opened blocks.
        if "```json-chart" not in text:
            return text

        out = []
        i = 0
        needle = "```json-chart"
        while True:
            idx = text.find(needle, i)
            if idx == -1:
                out.append(text[i:])
                break

            # append preceding text
            out.append(text[i:idx])

            # keep the opening fence line
            fence_line_end = text.find("\n", idx)
            if fence_line_end == -1:
                out.append(text[idx:])
                break
            fence_line_end += 1
            out.append(text[idx:fence_line_end])

            # attempt to find end of JSON object
            json_end = find_json_end(text, fence_line_end)
            if json_end is None:
                # cannot repair; keep rest and stop
                out.append(text[fence_line_end:])
                break

            # include JSON object (up to closing brace)
            out.append(text[fence_line_end:json_end + 1])

            # check if there's already a closing fence soon after
            after_json = text[json_end + 1:]
            closing_idx = after_json.find("```")
            opening_idx2 = after_json.find(needle)

            if closing_idx != -1 and (opening_idx2 == -1 or closing_idx < opening_idx2):
                # existing closing fence; keep everything up to it as-is
                out.append(after_json[:closing_idx + 3])
                i = json_end + 1 + closing_idx + 3
                continue

            # missing closing fence: insert one
            out.append("\n```\n")
            i = json_end + 1

        return "".join(out)

    def _cluster_signals(self, signals: List[Dict[str, Any]], user_query: str = None) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Planner å°†ä¿¡å·èšç±»ä¸ºå‡ ä¸ªæ ¸å¿ƒä¸»é¢˜
        è¿”å›: [{"theme_title": "ä¸»é¢˜A", "signal_ids": [1, 2], "rationale": "..."}]
        """
        # å‡†å¤‡ç®€è¦è¾“å…¥
        signals_preview = ""
        for i, s in enumerate(signals, 1):
            title = s.title if hasattr(s, 'title') else s.get('title', '')
            signals_preview += f"[{i}] {title}\n"
            
        logger.info(f"ğŸ§  Clustering {len(signals)} signals into themes...")
        
        instruction = get_cluster_planner_instructions(signals_preview, user_query)
        self.planner.instructions = [instruction]
        
        try:
            response = self.planner.run(get_cluster_task(signals_preview))
            content = response.content
            
            cluster_data = extract_json(content)
            if cluster_data and "clusters" in cluster_data:
                clusters = cluster_data["clusters"]
                logger.info(f"âœ… Created {len(clusters)} signal clusters.")
                return clusters
            else:
                logger.warning("âš ï¸ Failed to parse cluster JSON, fallback to individual signal mode.")
                return []
                
        except Exception as e:
            logger.error(f"Signal clustering failed: {e}")
            return []

    @staticmethod
    def build_structured_report(report_md: str, signals: List[Dict[str, Any]], clusters: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºç»“æ„åŒ–ç ”æŠ¥è¾“å‡ºï¼ˆä¾¿äºå‰ç«¯æ¸²æŸ“ï¼‰"""
        text = (report_md or "").strip()
        lines = text.splitlines() if text else []

        # æ ‡é¢˜
        title = "ç ”æŠ¥"
        for line in lines:
            if line.startswith("# "):
                title = line.replace("# ", "").strip()
                break

        # ç« èŠ‚è§£æ
        sections: List[Dict[str, Any]] = []
        current: Dict[str, Any] | None = None
        for line in lines:
            heading = re.match(r"^(#{2,4})\s+(.*)$", line.strip())
            if heading:
                if current:
                    sections.append(current)
                current = {"title": heading.group(2).strip(), "content": []}
                continue
            if current is None:
                current = {"title": "æ‘˜è¦", "content": []}
            current["content"].append(line)
        if current:
            sections.append(current)

        # æ‘˜è¦è¦ç‚¹
        bullets = [
            re.sub(r"^[-*â€¢]\s+", "", l.strip())
            for l in lines
            if l.strip().startswith(("- ", "* ", "â€¢ "))
        ]
        bullets = [b for b in bullets if b]

        # ä¿¡å·æ˜ å°„
        signal_map = {}
        for i, s in enumerate(signals, 1):
            title_s = s.title if hasattr(s, "title") else s.get("title", "")
            signal_map[i] = {
                "id": i,
                "title": title_s,
                "summary": getattr(s, "summary", "") if not isinstance(s, dict) else s.get("summary", ""),
                "sentiment_score": getattr(s, "sentiment_score", None) if not isinstance(s, dict) else s.get("sentiment_score"),
                "confidence": getattr(s, "confidence", None) if not isinstance(s, dict) else s.get("confidence"),
                "intensity": getattr(s, "intensity", None) if not isinstance(s, dict) else s.get("intensity"),
                "impact_tickers": getattr(s, "impact_tickers", []) if not isinstance(s, dict) else s.get("impact_tickers", []),
                "expected_horizon": getattr(s, "expected_horizon", "") if not isinstance(s, dict) else s.get("expected_horizon", "")
            }

        # èšç±»
        structured_clusters = []
        for c in clusters or []:
            ids = c.get("signal_ids", []) or []
            structured_clusters.append({
                "title": c.get("theme_title", ""),
                "rationale": c.get("rationale", ""),
                "signal_ids": ids,
                "signals": [signal_map.get(i) for i in ids if i in signal_map]
            })

        return {
            "title": title,
            "summary_bullets": bullets[:8],
            "sections": [
                {"title": s["title"], "content": "\n".join(s["content"]).strip()}
                for s in sections
            ],
            "clusters": structured_clusters,
            "signals": list(signal_map.values())
        }

    def generate_report(self, signals: List[Dict[str, Any]], user_query: str = None) -> str:
        """
        æ‰§è¡Œ Write-Plan-Edit æµç¨‹ç”Ÿæˆç ”æŠ¥
        """
        stock_tools = StockTools(self.db, auto_update=False)

        logger.info(f"ğŸ“ Starting report generation for {len(signals)} signals...")
        
        # --- Phase 1: Signal Clustering ---
        clusters = self._cluster_signals(signals, user_query)
        
        # å¦‚æœèšç±»å¤±è´¥ï¼Œæˆ–è€…æ²¡æœ‰è¿”å› clustersï¼Œåˆ™å›é€€åˆ°æ¯ä¸ªä¿¡å·ä¸€èŠ‚ï¼ˆæ¨¡æ‹Ÿæ¯ä¸ªä¿¡å·æ˜¯ä¸€ä¸ªç°‡ï¼‰
        if not clusters:
             clusters = [{"theme_title": (s.title if hasattr(s, 'title') else s.get('title', '')), "signal_ids": [i]} for i, s in enumerate(signals, 1)]

        # Build stable bibliography keys first so Writer can cite deterministically
        bib_entries, signal_to_keys = self._build_bibliography(signals)
        key_to_num = {e.get("key"): i for i, e in enumerate(bib_entries, 1) if e.get("key")}

        # --- Phase 2: Writing Drafts based on Clusters ---
        sections = []
        sources_list_lines = []
        section_titles = []  # å­˜å‚¨ (anchor, title)

        # Sources list shown to the LLM (even though final references are injected programmatically)
        for entry in bib_entries:
            sources_list_lines.append(
                f"[@{entry.get('key')}] {entry.get('title')} ({entry.get('source')}), {entry.get('url') or 'N/A'}"
            )
        
        for i, cluster in enumerate(clusters, 1):
            theme_title = cluster.get("theme_title", f"ä¸»é¢˜ {i}")
            signal_ids = cluster.get("signal_ids", [])
            rationale = cluster.get("rationale", "")
            
            logger.info(f"âœï¸ Writing draft for theme [{i}/{len(clusters)}]: {theme_title} (Signals: {signal_ids})...")
            
            # èšåˆè¯¥ç°‡ä¸‹çš„æ‰€æœ‰ä¿¡å·å†…å®¹
            cluster_signals_text = ""
            cluster_price_context = ""
            cluster_tickers_seen = set()
            
            for sig_idx in signal_ids:
                # æ³¨æ„ï¼šsignal_ids æ˜¯ 1-basedï¼Œè®¿é—® list éœ€è¦ -1
                if sig_idx < 1 or sig_idx > len(signals):
                    continue
                    
                signal = signals[sig_idx-1]
                
                # èšåˆä¿¡å·æ–‡æœ¬
                cluster_signals_text += format_signal_for_report(signal, sig_idx, cite_keys=signal_to_keys.get(sig_idx, [])) + "\n"
                
                # èšåˆè¡Œæƒ… Context (å»é‡)
                analysis_text = getattr(signal, 'analysis', '') if not isinstance(signal, dict) else signal.get('analysis', '')
                potential_tickers = list(set(re.findall(r'\b(\d{6})\b', analysis_text)))
                for t in potential_tickers:
                    if t not in cluster_tickers_seen:
                        cluster_tickers_seen.add(t)
                        # è·å–è¡Œæƒ…
                        try:
                            end_date = datetime.now().strftime("%Y-%m-%d")
                            start_date = (datetime.now() - timedelta(days=15)).strftime("%Y-%m-%d")
                            df_ctx = stock_tools.get_stock_price(t, start_date=start_date, end_date=end_date)
                            if not df_ctx.empty:
                                last_5 = df_ctx.tail(5)
                                prices_str = ", ".join([f"{row['date']}:{row['close']}" for _, row in last_5.iterrows()])
                                cluster_price_context += f"- {t}: {prices_str}\n"
                        except Exception as e:
                            logger.debug(f"Failed to get price context for ticker {t}: {e}")
                            continue

            # æ’°å†™å•èŠ‚è‰ç¨¿ (åŸºäºä¸»é¢˜)
            writer_instruction = get_report_writer_instructions(
                theme_title=theme_title,
                signal_cluster_text=cluster_signals_text,
                signal_indices=signal_ids,
                price_context=cluster_price_context,
                user_query=user_query
            )
            
            try:
                self.writer.instructions = [writer_instruction] 
                response = self.writer.run(get_writer_task(theme_title))
                content = response.content.strip()
                
                # å°è¯•æå–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
                lines = content.split('\n')
                title_line = lines[0].strip().replace('###', '').strip().replace('#', '')
                # å¦‚æœç¬¬ä¸€è¡Œå¤ªé•¿æˆ–è€…æ²¡æ ‡é¢˜ï¼Œå°±ç”¨ theme_title
                final_title = title_line if title_line and len(title_line) < 50 else theme_title
                
                # å­˜å‚¨åŸå§‹ç« èŠ‚ï¼Œå¸¦é”šç‚¹
                section_content = f"<a id=\"section-{i}\"></a>\n\n{content}\n"
                sections.append(section_content)
                section_titles.append((f"section-{i}", final_title))
                
            except Exception as e:
                logger.error(f"Failed to write section for theme {theme_title}: {e}")
        
        if not sections:
            return "âš ï¸ æ— æ³•ç”Ÿæˆç ”æŠ¥ï¼šæ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç« èŠ‚ã€‚"

        sources_list_text = "\n".join(sources_list_lines)
        
        # --- Decision Point: Incremental vs Global ---
        # å¦‚æœå¼€å¯å¢é‡ç¼–è¾‘ï¼Œæˆ–è€…å†…å®¹æ€»é•¿åº¦è¶…è¿‡é˜ˆå€¼ï¼ˆå¦‚ 80000 å­—ç¬¦ï¼‰ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼ä»¥é¿å…ä¸Šä¸‹æ–‡æº¢å‡º
        total_content_length = sum(len(s) for s in sections)
        use_incremental = self.incremental_edit or total_content_length > 80000
        
        if use_incremental:
            logger.info(f"ğŸ”„ Using INCREMENTAL editing mode (sections={len(sections)})...")
            final_response_content = self._incremental_edit(sections, sources_list_text, section_titles, bib_entries=bib_entries, signal_to_keys=signal_to_keys)
        else:
            # --- Phase 3: Global Planning (The Planner) ---
            # è™½ç„¶å·²ç»èšç±»ï¼Œä½†å…¨å±€ Planner ä»æœ‰åŠ©äºè°ƒæ•´ç« èŠ‚é¡ºåºå’Œè¯†åˆ«åˆ†æ­§
            logger.info("ğŸ§  Using GLOBAL Planning & Editing mode...")
            
            # ... (Rest of global logic remains mostly the same, just operating on theme sections)
            draft_docs = []
            toc_lines = []
            for i, section in enumerate(sections, 1):
                title = section_titles[i-1][1]
                draft_docs.append({
                    "id": str(i),
                    "title": title,
                    "content": section,
                    "summary": section[:500]
                })
                toc_lines.append(f"[{i}] {title}")
            
            self.rag.update_data(draft_docs)
            toc_text = "\n".join(toc_lines)
            
            planner_instruction = get_report_planner_instructions(toc_text, len(signals), user_query)
            self.planner.instructions = [planner_instruction]
            
            try:
                plan_response = self.planner.run(get_planner_task())
                report_plan = plan_response.content
                logger.info("âœ… Report plan generated.")
            except Exception as e:
                logger.error(f"Planning failed: {e}")
                report_plan = "ï¼ˆè§„åˆ’å¤±è´¥ï¼Œè¯·æŒ‰é»˜è®¤é¡ºåºç¼–æ’ï¼‰"

            # --- Phase 4: Final Editing (The Editor) ---
            logger.info("ğŸ¬ Editing final report based on plan...")
            
            all_drafts_text = "\n---\n".join(sections)
            editor_instruction = get_report_editor_instructions(all_drafts_text, report_plan, sources_list_text)
            self.editor.instructions = [editor_instruction]
            
            try:
                # ä½¿ç”¨ Editor è¿›è¡Œé‡ç»„å’Œæ¶¦è‰²
                final_response = self.editor.run(get_editor_task())
                final_response_content = final_response.content
            except Exception as e:
                logger.error(f"Final editing failed: {e}")
                final_response_content = f"# ç ”æŠ¥ç”Ÿæˆå¤±è´¥\n\n{e}"

            # Normalize citations + inject programmatic bibliography
            final_response_content = self._normalize_citations(final_response_content, signal_to_keys)
            final_response_content = self._inject_references(
                final_response_content,
                self._render_references_section(bib_entries, key_to_num),
            )

        # æ¸…ç† Markdown æ ‡è®°
        final_response_content = final_response_content.strip()
        if final_response_content.startswith("```markdown"):
            final_response_content = final_response_content[len("```markdown"):].strip()
        if final_response_content.startswith("```"):
            final_response_content = final_response_content[3:].strip()
        if final_response_content.endswith("```"):
            final_response_content = final_response_content[:-3].strip()

        # ç»Ÿä¸€æ·»åŠ  TOC (å¦‚æœ Editor æœªç”Ÿæˆ)
        if not use_incremental and "[TOC]" not in final_response_content:
             lines = final_response_content.split('\n')
             if lines and lines[0].strip().startswith('# '):
                 # æ’å…¥åœ¨æ ‡é¢˜ä¹‹å
                 final_response_content = lines[0] + "\n\n[TOC]\n\n" + "\n".join(lines[1:])
             else:
                 # æ’å…¥åœ¨æœ€å‰
                 final_response_content = "[TOC]\n\n" + final_response_content
        
        # Fix duplicate headers (e.g. "#### #### Title") caused by LLM stutter
        final_response_content = re.sub(r'(#{1,6})\s+\1', r'\1', final_response_content)

        # Normalize citations + inject programmatic bibliography (incremental path may also pass through here)
        final_response_content = self._normalize_citations(final_response_content, signal_to_keys, key_to_num)
        final_response_content = self._inject_references(
            final_response_content,
            self._render_references_section(bib_entries, key_to_num),
        )
        
        # --- Phase 5: Visualization Processing ---
        logger.info("ğŸ¨ Processing visualization...")

        # Repair malformed json-chart blocks (e.g. missing closing fence) before extraction/rendering
        final_response_content = self._sanitize_json_chart_blocks(final_response_content)

        forecast_map = self._build_forecast_map(final_response_content, signals)
        final_report_with_charts = self._process_charts(final_response_content, signals, forecast_map=forecast_map)

        structured_report = self.build_structured_report(final_response_content, signals, clusters)
        return SimpleNamespace(content=final_report_with_charts, structured=structured_report)

    def _clean_markdown(self, text: str) -> str:
        """Helper to remove markdown code fences"""
        text = text.strip()
        if text.startswith("```markdown"):
            text = text[len("```markdown"):].strip()
        elif text.startswith("```"):
            text = text[3:].strip()
        if text.endswith("```"):
            text = text[:-3].strip()
        return text

    def _incremental_edit(
        self,
        sections: List[str],
        sources_list_text: str,
        section_titles_data: List[tuple] = None,
        bib_entries: Optional[list[Dict[str, Any]]] = None,
        signal_to_keys: Optional[Dict[int, list[str]]] = None,
    ) -> str:
        """å¢é‡ç¼–è¾‘æ¨¡å¼"""
        # 1. å¡«å…… RAG
        draft_docs = []
        toc_lines = []
        for i, section in enumerate(sections, 1):
            if section_titles_data and i <= len(section_titles_data):
                _, title = section_titles_data[i-1]
            else:
                title = f"ç« èŠ‚ {i}"
            
            draft_docs.append({
                "id": str(i),
                "title": title,
                "content": section,
                "summary": section[:300]
            })
            toc_lines.append(f"[{i}] {title}")
        
        self.rag.update_data(draft_docs)
        toc = "\n".join(toc_lines)
        
        # 2. é€èŠ‚ç¼–è¾‘
        edited_sections = []
        for i, section in enumerate(sections, 1):
            logger.info(f"âœï¸ Incremental editing: section {i}/{len(sections)}...")
            
            editor_instruction = get_section_editor_instructions(i, len(sections), toc)
            self.section_editor.instructions = [editor_instruction]
            
            try:
                response = self.section_editor.run(f"è¯·ç¼–è¾‘ä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼š\n\n{section}")
                cleaned_content = self._clean_markdown(response.content)
                edited_sections.append(cleaned_content)
            except Exception as e:
                logger.warning(f"âš ï¸ Section {i} editing failed: {e}, using original")
                edited_sections.append(self._clean_markdown(section))
            
            # ç®€çŸ­å»¶è¿Ÿé¿å… API è¿‡è½½
            time.sleep(0.5)
        
        # 3. ç”Ÿæˆæ‘˜è¦
        logger.info("ğŸ“ Generating summary (incremental)...")
        section_summaries = "\n".join([s[:200] + "..." for s in edited_sections])
        summary_instruction = get_summary_generator_instructions(toc, section_summaries)
        self.editor.instructions = [summary_instruction]
        
        try:
            summary_response = self.editor.run("è¯·ç”Ÿæˆæ ¸å¿ƒè§‚ç‚¹æ‘˜è¦ã€‚")
            summary = self._clean_markdown(summary_response.content)
        except Exception as e:
            logger.warning(f"âš ï¸ Summary generation failed: {e}")
            summary = "ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œè¯·å‚é˜…å„ç« èŠ‚è¯¦æƒ…ã€‚ï¼‰"
        
        # 4. ç”Ÿæˆå‚è€ƒæ–‡çŒ®å’Œå°¾éƒ¨å†…å®¹
        logger.info("ğŸ“š Generating references (incremental)...")
        assembly_instruction = get_final_assembly_instructions(sources_list_text)
        self.editor.instructions = [assembly_instruction]
        
        try:
            tail_response = self.editor.run("è¯·ç”Ÿæˆå‚è€ƒæ–‡çŒ®ã€é£é™©æç¤ºå’Œå¿«é€Ÿæ‰«æè¡¨æ ¼ã€‚")
            tail_content = self._clean_markdown(tail_response.content)
            # Some models (or fallback templates) may accidentally indent headings, turning them into code blocks.
            tail_content = re.sub(r'(?m)^[ \t]+(#{1,6}\s+)', r'\1', tail_content)
            # And sometimes they indent whole sections (e.g. 12 spaces). Tail is expected to be prose/tables, not code.
            tail_content = re.sub(r'(?m)^[ \t]{4,}(?=\S)', '', tail_content)

            # Guardrail: some models ask the user for more info instead of generating the required sections.
            bad_markers = ["ä¸ºäº†å®Œæˆæ‚¨çš„è¯·æ±‚", "æˆ‘éœ€è¦æ‚¨æä¾›", "è¯·æ‚¨æä¾›", "è¯·æä¾›å¿…è¦çš„ç»†èŠ‚"]
            if any(m in tail_content for m in bad_markers) or ("å‚è€ƒæ–‡çŒ®" not in tail_content and "é£é™©æç¤º" not in tail_content):
                raise ValueError("Tail content looks invalid; falling back")
            
            # åˆ†ç¦»å¿«é€Ÿæ‰«æå’Œå…¶ä»–å°¾éƒ¨å†…å®¹
            quick_scan = ""
            other_tail = tail_content
            if "å¿«é€Ÿæ‰«æ" in tail_content:
                parts = tail_content.split("## å¿«é€Ÿæ‰«æ")
                if len(parts) == 2:
                    other_tail = parts[0].strip()
                    quick_scan = "## å¿«é€Ÿæ‰«æ" + parts[1].split("## ")[0] if "## " in parts[1] else "## å¿«é€Ÿæ‰«æ" + parts[1]
        except Exception as e:
            logger.warning(f"âš ï¸ Tail content generation failed: {e}")
            quick_scan = ""
            sources_clean = (sources_list_text or "").strip()
            other_tail = (
                "## å‚è€ƒæ–‡çŒ®\n\n"
                + (sources_clean + "\n\n" if sources_clean else "ï¼ˆæ— ï¼‰\n\n")
                + "## é£é™©æç¤º\n\n"
                + "æœ¬æŠ¥å‘Šç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚\n"
            )

        # Programmatically inject references to avoid LLM instability
        try:
            bib_entries_safe = bib_entries or []
            key_to_num = {e.get("key"): i for i, e in enumerate(bib_entries_safe, 1) if e.get("key")}
            other_tail = self._inject_references(other_tail, self._render_references_section(bib_entries_safe, key_to_num))
        except Exception as e:
            logger.debug(f"Failed to inject references programmatically: {e}")
        
        # 5. ç»„è£…æœ€ç»ˆæŠ¥å‘Š
        current_date = datetime.now().strftime('%Y-%m-%d')
        
        # æ¸…ç† edited_sectionsï¼šåªåšä»£ç å—ä¿æŠ¤å’ŒåŸºæœ¬æ¸…ç†
        
        # æ¸…ç† edited_sections ä¸­çš„æ ‡é¢˜å±‚çº§é—®é¢˜
        cleaned_sections = []
        for section in edited_sections:
            # ä¿æŠ¤ä»£ç å—ï¼šå…ˆä¸´æ—¶æ›¿æ¢ä»£ç å—å†…å®¹
            code_blocks = []
            def preserve_code_block(match):
                code_blocks.append(match.group(0))
                return f"__CODE_BLOCK_{len(code_blocks) - 1}__"
            
            section_protected = re.sub(r'```[\s\S]*?```', preserve_code_block, section)
            
            # åªæ¸…ç†æ˜æ˜¾çš„é”™è¯¯ï¼šé‡å¤çš„ # ç¬¦å·ï¼ˆLLM stutterï¼‰
            # ç§»é™¤é‡å¤çš„ # ç¬¦å·
            section_fixed = re.sub(r'(#{1,6})\s+\1+', r'\1', section_protected)
            
            # æ¢å¤ä»£ç å—
            for i, block in enumerate(code_blocks):
                section_fixed = section_fixed.replace(f"__CODE_BLOCK_{i}__", block)
            
            cleaned_sections.append(section_fixed)
        
        # Use simple string concatenation or 0-indented string to avoid dedent issues with dynamic content
        final_report = f"""# AlphaEar å…¨çƒå¸‚åœºè¶‹åŠ¿æ—¥æŠ¥ ({current_date})

[TOC]

{quick_scan}

{summary}

{"\n\n".join(cleaned_sections)}

{other_tail}
"""
        # Fix duplicate headers (e.g. "#### #### Title") caused by LLM stutter
        final_report = re.sub(r'(#{1,6})\s+\1', r'\1', final_report)

        # Normalize citations for final report
        bib_entries_safe = bib_entries or []
        key_to_num = {e.get("key"): i for i, e in enumerate(bib_entries_safe, 1) if e.get("key")}
        final_report = self._normalize_citations(final_report, signal_to_keys or {}, key_to_num)
        
        # ç§»é™¤è¿ç»­çš„ç©ºè¡Œï¼ˆæœ€å¤šä¿ç•™2ä¸ªï¼‰
        final_report = re.sub(r'\n{4,}', '\n\n\n', final_report)
         
        return final_report.strip()
    

    def _process_charts(self, content: str, signals: List[Dict[str, Any]] = None, forecast_map: Optional[Dict[tuple, ForecastResult]] = None) -> str:
        """è§£æ json-chart ä»£ç å—å¹¶æ›¿æ¢ä¸º HTML é“¾æ¥/Iframe"""
        from utils.visualizer import VisualizerTools
        from utils.stock_tools import StockTools
        
        stock_tools = StockTools(self.db, auto_update=False)

        # Cache rendered forecast HTML per (ticker, pred_len) to guarantee identical output across duplicates
        rendered_forecast_html: Dict[tuple, str] = {}

        def replace_match(match):
            json_str = match.group(1).strip()
            # Normalize smart quotes that frequently break JSON parsing.
            json_str = (
                json_str.replace("\u201c", '"')
                .replace("\u201d", '"')
                .replace("\u2018", "'")
                .replace("\u2019", "'")
                .replace("â€œ", '"')
                .replace("â€", '"')
                .replace("â€˜", "'")
                .replace("â€™", "'")
            )
            try:
                config = extract_json(json_str)
                if not config:
                    raise ValueError("No valid JSON found in chart block")
                
                chart_type = config.get("type")
                
                if chart_type == "stock":
                    ticker_raw = config.get("ticker", "")
                    base_title = config.get("title", f"{ticker_raw} èµ°åŠ¿")
                    prediction = config.get("prediction", None)
                    
                    # å¤„ç†å¤šä¸ª ticker çš„æƒ…å†µï¼ˆé€—å·æˆ–ç©ºæ ¼åˆ†éš”ï¼‰
                    tickers = re.split(r'[,\s]+', str(ticker_raw).strip())
                    
                    # å°è¯•è§£ææ¯ä¸ª ticker
                    valid_tickers = []
                    for t in tickers:
                        t = t.strip()
                        if not t:
                            continue
                        
                        # 1. é¢„å¤„ç†ï¼šç§»é™¤åç¼€
                        clean_t = t.split('.')[0] if '.' in t else t
                        
                        # 2. ç›´æ¥åŒ¹é…ï¼š5ä½(æ¸¯è‚¡) æˆ– 6ä½(Aè‚¡) æ•°å­—ä»£ç 
                        if clean_t.isdigit() and (len(clean_t) == 5 or len(clean_t) == 6):
                            valid_tickers.append(clean_t)
                            logger.info(f"ğŸ“Š Extracted ticker {clean_t} from {t}")
                            continue

                        # 3. å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†åç§°ã€çŸ­ä»£ç ç­‰ï¼‰
                        if len(t) > 1 or (clean_t.isdigit() and len(clean_t) < 5):
                            try:
                                search_results = stock_tools.search_ticker(t)
                                if search_results and len(search_results) > 0:
                                    best_match = None
                                    
                                    # æ™ºèƒ½åŒ¹é…é€»è¾‘
                                    if clean_t.isdigit():
                                        # æ„é€ å¯èƒ½çš„å®Œæ•´ä»£ç 
                                        candidates = []
                                        # å¦‚æœæ˜ç¡®æ˜¯ HK åç¼€ï¼Œä¼˜å…ˆåŒ¹é… 5 ä½è¡¥é›¶
                                        if '.HK' in t.upper():
                                            candidates.append(clean_t.zfill(5))
                                        # å¦‚æœæ˜ç¡®æ˜¯ A è‚¡åç¼€ï¼Œä¼˜å…ˆåŒ¹é… 6 ä½è¡¥é›¶
                                        elif '.SZ' in t.upper() or '.SH' in t.upper():
                                            candidates.append(clean_t.zfill(6))
                                        else:
                                            # æ— åç¼€ï¼Œéƒ½å°è¯•ï¼Œä¼˜å…ˆ 5 ä½ (æ¸¯è‚¡çŸ­ç å¸¸è§)ï¼Œç„¶å 6 ä½
                                            candidates.append(clean_t.zfill(5))
                                            candidates.append(clean_t.zfill(6))
                                        
                                        # åœ¨æœç´¢ç»“æœä¸­å¯»æ‰¾å®Œå…¨åŒ¹é…
                                        for cand in candidates:
                                            for res in search_results:
                                                if res['code'] == cand:
                                                    best_match = res['code']
                                                    break
                                            if best_match: break
                                    
                                    # å¦‚æœæ²¡æœ‰é€šè¿‡æ•°å­—è¡¥å…¨æ‰¾åˆ°ï¼Œå°è¯•åç§°åŒ¹é…æˆ–é»˜è®¤ç¬¬ä¸€ä¸ª
                                    if not best_match:
                                        # å†æ¬¡éå†ï¼Œçœ‹æœ‰æ²¡æœ‰å®Œå…¨ç­‰äº clean_t çš„ (è™½ç„¶å‰é¢ digit check åº”è¯¥è¦†ç›–äº†)
                                        for res in search_results:
                                            if res['code'] == clean_t:
                                                best_match = res['code']
                                                break
                                    
                                    final_ticker = best_match if best_match else search_results[0].get('code', '')
                                    
                                    if final_ticker:
                                        valid_tickers.append(final_ticker)
                                        logger.info(f"ğŸ“Š Fuzzy matched ticker {final_ticker} from query '{t}'")
                            except Exception as e:
                                logger.warning(f"âš ï¸ Fuzzy search failed for {t}: {e}")
                    
                    tickers = valid_tickers
                    
                    if not tickers:
                        logger.warning(f"âš ï¸ No valid ticker found in: {ticker_raw}")
                        return f"\n<!-- æ— æ³•è§£æè‚¡ç¥¨ä»£ç : {ticker_raw} -->\n"

                    
                    if len(tickers) > 1:
                        logger.info(f"ğŸ“Š Multiple tickers detected: {tickers}, generating charts for all")
                    
                    # ä¸ºæ¯ä¸ª ticker ç”Ÿæˆå›¾è¡¨
                    all_charts_html: List[str] = []
                    end_date = datetime.now().strftime("%Y-%m-%d")
                    start_date = (datetime.now() - timedelta(days=90)).strftime("%Y-%m-%d")
                    
                    for idx, ticker in enumerate(tickers):
                        # å¦‚æœæœ‰å¤šä¸ª tickerï¼Œä¸ºæ¯ä¸ªç”Ÿæˆç‹¬ç«‹çš„æ ‡é¢˜
                        if len(tickers) > 1:
                            chart_title = f"{ticker} - {base_title}"
                        else:
                            chart_title = base_title
                        
                        df = stock_tools.get_stock_price(ticker, start_date=start_date, end_date=end_date)
                        
                        if not df.empty:
                            # Optional: attach Kronos forecast if explicitly requested
                            forecast_obj = None
                            if config.get("show_forecast", False) or config.get("forecast", False):
                                try:
                                    related_signals = []
                                    if signals:
                                        for s in signals:
                                            analysis_text = getattr(s, 'analysis', '') if not isinstance(s, dict) else s.get('analysis', '')
                                            title_text = getattr(s, 'title', '') if not isinstance(s, dict) else s.get('title', '')
                                            full_text = f"{title_text} {analysis_text}"
                                            if str(ticker) in full_text:
                                                related_signals.append(s)
                                    forecast_obj = self._get_forecast_agent().generate_forecast(ticker, related_signals)
                                except Exception as e:
                                    logger.warning(f"âš ï¸ Forecast generation failed for {ticker}: {e}")
                                    forecast_obj = None

                            chart = VisualizerTools.generate_stock_chart(
                                df,
                                ticker,
                                chart_title,
                                prediction=prediction,
                                forecast=forecast_obj
                            )

                            if chart:
                                timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                                filename = f"reports/charts/stock_{ticker}_{timestamp}.html"
                                VisualizerTools.render_chart_to_file(chart, filename)
                                rel_path = f"charts/stock_{ticker}_{timestamp}.html"
                                all_charts_html.append(
                                    f'<iframe src="{rel_path}" width="100%" height="500px" style="border:none;"></iframe>\n'
                                    f'<p style="text-align:center;color:gray;font-size:12px">äº¤äº’å¼å›¾è¡¨: {chart_title}</p>'
                                )
                        else:
                            logger.warning(f"âš ï¸ No data for ticker: {ticker}")
                    
                    if all_charts_html:
                        return "\n" + "\n".join(all_charts_html) + "\n"
                    else:
                        return f"\n<!-- æ— æ³•è·å–è‚¡ç¥¨æ•°æ®: {ticker_raw} -->\n"

                elif chart_type == "forecast":
                    ticker_raw = config.get("ticker", "")
                    title = config.get("title", f"{ticker_raw} é¢„æµ‹")
                    pred_len = config.get("pred_len", 5)
                    
                    # Only allow one ticker for forecast (supports suffix like 002371.SZ / 9868.HK)
                    t = str(ticker_raw).strip().split(',')[0].strip()
                    clean_t = t.split('.')[0] if '.' in t else t
                    clean_t = ''.join([c for c in clean_t if c.isdigit()]) or clean_t
                    if not (clean_t.isdigit() and len(clean_t) in (5, 6)):
                        return (
                            f'\n<p style="text-align:center;color:#b45309;font-size:13px;'
                            f'background:#fffbeb;padding:10px;border:1px dashed #f59e0b;border-radius:8px;">'
                            f'âš ï¸ æš‚ä¸æ”¯æŒè¯¥è‚¡ç¥¨ä»£ç çš„é¢„æµ‹æ¸²æŸ“ï¼š{ticker_raw}ï¼ˆä»…æ”¯æŒ A è‚¡ 6 ä½ / æ¸¯è‚¡ 5 ä½æ•°å­—ä»£ç ï¼‰ã€‚'
                            f'</p>\n'
                        )
                    ticker = clean_t
                    
                    # Gather signals that mention this ticker
                    related_signals = []
                    if signals:
                        for s in signals:
                            # è¾…åŠ©å‡½æ•°ï¼šä»ä¿¡å·ä¸­æå–æ‰€æœ‰ç›¸å…³çš„ ticker
                            # å…¼å®¹å­—å…¸å’Œ Pydantic æ¨¡å‹
                            analysis_text = getattr(s, 'analysis', '') if not isinstance(s, dict) else s.get('analysis', '')
                            title_text = getattr(s, 'title', '') if not isinstance(s, dict) else s.get('title', '')
                            full_text = f"{title_text} {analysis_text}"
                            if ticker in full_text:
                                related_signals.append(s)
                    
                    key = (ticker, int(pred_len) if str(pred_len).isdigit() else 5)

                    if key in rendered_forecast_html:
                        return rendered_forecast_html[key]

                    forecast_obj = None
                    if forecast_map and key in forecast_map:
                        forecast_obj = forecast_map[key]
                    else:
                        # Backward-compatible fallback (may be inconsistent across duplicates)
                        forecast_obj = self._get_forecast_agent().generate_forecast(ticker, related_signals, pred_len=pred_len)
                    
                    # Fetch history for rendering
                    end_date = datetime.now().strftime("%Y-%m-%d")
                    start_date = (datetime.now() - timedelta(days=60)).strftime("%Y-%m-%d")
                    df = stock_tools.get_stock_price(ticker, start_date=start_date, end_date=end_date)
                    
                    if df.empty:
                        html = f"<!-- æ— æ³•è·å–è‚¡ç¥¨æ•°æ®: {ticker} -->"
                        rendered_forecast_html[key] = html
                        return html

                    if forecast_obj:
                        chart = VisualizerTools.generate_stock_chart(df, ticker, title, forecast=forecast_obj)
                        if chart:
                            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                            filename = f"reports/charts/forecast_{ticker}_{timestamp}.html"
                            VisualizerTools.render_chart_to_file(chart, filename)

                            rel_path = f"charts/forecast_{ticker}_{timestamp}.html"
                            html = (
                                f'<iframe src="{rel_path}" width="100%" height="500px" style="border:none;"></iframe>\n'
                                f'<p style="text-align:center;color:gray;font-size:12px">AI æ·±åº¦é¢„æµ‹: {title}</p>'
                            )
                            html += (
                                f'\n<p style="font-size:13px; color:#555; background:#f9f9f9; padding:10px; '
                                f'border-left:4px solid #9333ea;"><b>é¢„æµ‹é€»è¾‘:</b> {forecast_obj.rationale}</p>\n'
                            )
                            rendered_forecast_html[key] = html
                            return html

                    # Fallback: forecast failed, still render history-only chart
                    chart = VisualizerTools.generate_stock_chart(df, ticker, title)
                    if chart:
                        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                        filename = f"reports/charts/stock_{ticker}_{timestamp}.html"
                        VisualizerTools.render_chart_to_file(chart, filename)
                        rel_path = f"charts/stock_{ticker}_{timestamp}.html"
                        html = (
                            f'<iframe src="{rel_path}" width="100%" height="500px" style="border:none;"></iframe>\n'
                            f'<p style="text-align:center;color:gray;font-size:12px">ï¼ˆé¢„æµ‹å¤±è´¥ï¼Œå·²å±•ç¤ºå†å²è¡Œæƒ…ï¼‰{title}</p>'
                        )
                        rendered_forecast_html[key] = html
                        return html

                    html = f"<!-- FORECAST FAILED FOR {ticker} -->"
                    rendered_forecast_html[key] = html
                    return html



                
                elif chart_type == "sentiment":
                    keywords = config.get("keywords", [])
                    title = config.get("title", "èˆ†æƒ…æƒ…ç»ªè¶‹åŠ¿")
                    
                    if keywords:
                        # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢ SQL æ³¨å…¥
                        conditions = " OR ".join(["content LIKE ?" for _ in keywords])
                        params = tuple(f"%{k}%" for k in keywords)
                        query = f"SELECT publish_time, sentiment_score FROM daily_news WHERE ({conditions}) AND sentiment_score IS NOT NULL ORDER BY publish_time"
                        
                        logger.info(f"ğŸ“Š Executing sentiment query: {query} with {len(params)} params")
                        results = self.db.execute_query(query, params)
                        logger.info(f"ğŸ“Š Query result count: {len(results)}")
                        
                        if not results or len(results) == 0:
                            # Fallback: Try broadening search by splitting keywords
                            logger.info("âš ï¸ Initial sentiment query empty, attempting fallback with split keywords...")
                            broad_keywords = []
                            for k in keywords:
                                broad_keywords.extend(k.split())
                            
                            # Deduplicate and filter short words
                            broad_keywords = list(set([k for k in broad_keywords if len(k) > 1]))
                            
                            if broad_keywords:
                                conditions = " OR ".join(["content LIKE ?" for _ in broad_keywords])
                                params = tuple(f"%{k}%" for k in broad_keywords)
                                query = f"SELECT publish_time, sentiment_score FROM daily_news WHERE ({conditions}) AND sentiment_score IS NOT NULL ORDER BY publish_time"
                                logger.info(f"ğŸ“Š Executing fallback sentiment query: {query} with {len(params)} params")
                                results = self.db.execute_query(query, params)
                                logger.info(f"ğŸ“Š Fallback query result count: {len(results)}")

                        if results:
                            # æ ¼å¼åŒ–æ•°æ®
                            sentiment_history = []
                            for row in results:
                                try:
                                    # å‡è®¾ publish_time æ˜¯å­—ç¬¦ä¸²ï¼Œæˆ–è€… date object
                                    dt = row[0]
                                    if isinstance(dt, datetime):
                                        d_str = dt.strftime("%Y-%m-%d")
                                    else:
                                        d_str = str(dt)[:10] # æˆªå–æ—¥æœŸéƒ¨åˆ†
                                        
                                    sentiment_history.append({"date": d_str, "score": row[1]})
                                except (TypeError, ValueError, IndexError) as e:
                                    logger.debug(f"Failed to parse sentiment row: {e}")
                                    continue
                            
                            # èšåˆæ¯å¤©çš„å¹³å‡åˆ†
                            df_sent = pd.DataFrame(sentiment_history)
                            if not df_sent.empty:
                                df_sent = df_sent.groupby('date')['score'].mean().reset_index()
                                sentiment_history_agg = df_sent.to_dict('records')
                                
                                chart = VisualizerTools.generate_sentiment_trend_chart(sentiment_history_agg)
                                if chart:
                                    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                                    filename = f"reports/charts/sentiment_{timestamp}.html"
                                    VisualizerTools.render_chart_to_file(chart, filename)
                                    rel_path = f"charts/sentiment_{timestamp}.html"
                                    return f'\n<iframe src="{rel_path}" width="100%" height="400px" style="border:none;"></iframe>\n<p style="text-align:center;color:gray;font-size:12px">äº¤äº’å¼å›¾è¡¨: {title}</p>\n'
                        
                        # Fallback for sentiment if query results are empty
                        return f'\n<p style="text-align:center;color:gray;font-size:12px;padding:20px;border:1px dashed #ccc;border-radius:8px;">ğŸ“Š æš‚æ— è¶³å¤Ÿå†å²æ•°æ®ç”Ÿæˆ "{title}" çš„è¶‹åŠ¿å›¾</p>\n'

                elif chart_type == "isq":
                    sentiment = config.get("sentiment", 0.0)
                    confidence = config.get("confidence", 0.5)
                    intensity = config.get("intensity", 3)
                    expectation_gap = config.get("expectation_gap", 0.5)
                    timeliness = config.get("timeliness", 0.8)
                    title = config.get("title", "ä¿¡å·è´¨é‡ ISQ è¯„ä¼°")
                    
                    chart = VisualizerTools.generate_isq_radar_chart(
                        sentiment, confidence, intensity, 
                        expectation_gap=expectation_gap, 
                        timeliness=timeliness, 
                        title=title
                    )
                    if chart:
                        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                        # Avoid collisions: multiple ISQ charts can be rendered within the same second.
                        payload = {
                            "type": "isq",
                            "sentiment": sentiment,
                            "confidence": confidence,
                            "intensity": intensity,
                            "expectation_gap": expectation_gap,
                            "timeliness": timeliness,
                            "title": title,
                        }
                        content_hash = hashlib.md5(
                            json.dumps(payload, sort_keys=True, ensure_ascii=False).encode("utf-8")
                        ).hexdigest()[:8]
                        filename = f"reports/charts/isq_{timestamp}_{content_hash}.html"
                        VisualizerTools.render_chart_to_file(chart, filename)
                        rel_path = f"charts/isq_{timestamp}_{content_hash}.html"
                        return f'\n<iframe src="{rel_path}" width="100%" height="420px" style="border:none;"></iframe>\n<p style="text-align:center;color:gray;font-size:12px">ä¿¡å·è´¨é‡é›·è¾¾å›¾: {title}</p>\n'

                elif chart_type == "transmission":
                    nodes = config.get("nodes", [])
                    title = config.get("title", "æŠ•èµ„é€»è¾‘ä¼ å¯¼é“¾æ¡")
                    
                    if nodes:
                        # ç”ŸæˆåŸºäºèŠ‚ç‚¹å†…å®¹çš„å”¯ä¸€æ ‡è¯†
                        nodes_str = json.dumps(nodes, sort_keys=True, ensure_ascii=False)
                        content_hash = hashlib.md5(nodes_str.encode()).hexdigest()[:8]
                        
                        # Generate XML using LLM with retry
                        max_retries = 2
                        xml_success = False
                        
                        for attempt in range(max_retries):
                            try:
                                from prompts.visualizer import get_drawio_system_prompt, get_drawio_task
                                
                                # Use tool_model (usually faster/cheaper) or main model
                                # Creating a lightweight agent purely for XML generation
                                visualizer_agent = Agent(
                                    model=self.tool_model,
                                    instructions=[get_drawio_system_prompt()],
                                    markdown=False
                                )
                                
                                logger.info(f"ğŸ¨ Generating Draw.io XML for '{title}' (attempt {attempt + 1}/{max_retries})...")
                                resp = visualizer_agent.run(get_drawio_task(nodes, title))
                                xml_content = resp.content
                                
                                # Basic cleanup if LLM wrapped in markdown code blocks
                                match = re.search(r'<mxGraphModel.*?</mxGraphModel>', xml_content, re.DOTALL)
                                if match:
                                    xml_content = match.group(0)
                                    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                                    filename = f"reports/charts/trans_{timestamp}_{content_hash}.html"
                                    
                                    result_path = VisualizerTools.render_drawio_to_html(xml_content, filename, title)
                                    if result_path:
                                        rel_path = f"charts/trans_{timestamp}_{content_hash}.html"
                                        xml_success = True
                                        return f'\n<iframe src="{rel_path}" width="100%" height="500px" style="border:none;"></iframe>\n<p style="text-align:center;color:gray;font-size:12px">äº¤äº’å¼é€»è¾‘æ¨æ¼”å›¾: {title} (AI Generated)</p>\n'
                                    else:
                                        logger.warning(f"âš ï¸ Render failed for {title}, attempt {attempt + 1}")
                                else:
                                    logger.warning(f"âš ï¸ Failed to extract XML from response for {title}, attempt {attempt + 1}")
                                    
                            except Exception as e:
                                logger.error(f"Draw.io generation failed (attempt {attempt + 1}): {e}")
                            
                            # Wait before retry
                            if attempt < max_retries - 1:
                                import time
                                time.sleep(1)
                        
                        # Fallback mechanism (Old Graph) if all retries failed
                        if not xml_success:
                            logger.info("âš ï¸ Falling back to Pyecharts Graph for Transmission Chain.")
                            chart = VisualizerTools.generate_transmission_graph(nodes, title)
                            if chart:
                                timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                                filename = f"reports/charts/trans_legacy_{timestamp}_{content_hash}.html"
                                VisualizerTools.render_chart_to_file(chart, filename)
                                rel_path = f"charts/trans_legacy_{timestamp}_{content_hash}.html"
                                return f'\n<iframe src="{rel_path}" width="100%" height="420px" style="border:none;"></iframe>\n<p style="text-align:center;color:gray;font-size:12px">é€»è¾‘ä¼ å¯¼æ‹“æ‰‘å›¾: {title}</p>\n'

                # å¦‚æœæ˜¯å…¶ä»–ç±»å‹æˆ–å¤±è´¥ï¼Œä¿ç•™åŸæ–‡æˆ–è€…æ˜¾ç¤ºé”™è¯¯
                return f"```json\n{json_str}\n```" # Fallback to json display if render fails logic mismatch
            
            except Exception as e:
                logger.error(f"Chart processing failed: {e}")
                return match.group(0) # Return original text on error

        # åŒ¹é… ```json-chart ... ```
        pattern = re.compile(r'```json-chart\s*(\{.*?\})\s*```', re.DOTALL)
        new_content = pattern.sub(replace_match, content)

        # Make invalid-forecast-ticker failures visible (older versions emitted HTML comments)
        new_content = re.sub(
            r'<!--\s*NO VALID TICKER FOR FORECAST:\s*([^>]+?)\s*-->',
            lambda m: (
                '\n<p style="text-align:center;color:#b45309;font-size:13px;'
                'background:#fffbeb;padding:10px;border:1px dashed #f59e0b;border-radius:8px;">'
                f'âš ï¸ æš‚ä¸æ”¯æŒè¯¥è‚¡ç¥¨ä»£ç ï¼š{m.group(1).strip()}ã€‚'
                '</p>\n'
            ),
            new_content,
        )

        return new_content
